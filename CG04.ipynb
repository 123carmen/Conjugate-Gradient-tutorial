{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib widget\n",
    "from CG import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "A = np.matrix([[3.0, 2.0], [2.0, 6.0]])\n",
    "b = np.matrix([[2.0], [-8.0]])\n",
    "c = 0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Method of Steepest Descent <a id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the method of Steepest Descent, we start at an arbitrary point $x_{[0]}$ and slide down to the bottom of the paraboloid. We take a series of steps $x_{[1]}, x_{[2]}, \\dots$ until we are satisfied that we are close enough to the solution $x$.\n",
    "\n",
    "When we take a step, we choose the direction in which $f$ decreases most quickly, which is the direction opposite $f'(x_{[i]})$. According to Equation 7, this direction is $-f'(x_{[i]}) =  b - Ax_{[i]}$.\n",
    "\n",
    "Allow me to introduce a few definitions, which you should memorize. The *error* $e_{[i]} = x_{[i]} - x$ is a vector that indicates how far we are from the solution. The *residual* $r_{[i]} = b - Ax_{[i]}$ indicates how far we are from the correct value of $b$. It is easy to see that $r_{[i]} = -Ae_{[i]}$, and you should think of the residual as being the error transformed by $A$ into the same space as $b$. More importantly, $r_{[i]} = -f'(x_{[i]})$, and you should also think of the residual as the direction of steepest descent. For nonlinear problems, discussed in Section 14, only the latter definition applies. So remember, whenever you read “residual”, think “direction of steepest descent.”\n",
    "\n",
    "Suppose we start at $x_{[0]} = [-2, -2]^T$. Our first step, along the direction of steepest descent, will fall somewhere on the solid line in Figure 6(a). In other words, we will choose a point\n",
    "\n",
    "\\begin{equation}\n",
    "x_{[1]} = x_{[0]} + \\alpha r \\tag{9}\n",
    "\\end{equation}\n",
    "\n",
    "The question is, how big a step should we take?\n",
    "\n",
    "A line search is a procedure that chooses $\\alpha$ to minimize $f$ along a line. Figure 6(b) illustrates this task: we are restricted to choosing a point on the intersection of a vertical plane in direction $r$ and the paraboloid. Figure 6(c) is the parabola defined by the intersection of these surfaces. What is the value of at the base of the parabola?\n",
    "\n",
    "From basic calculus, $\\alpha$ minimizes $f$ when the *directional derivative* $\\frac{\\partial}{\\partial \\alpha}f(x_{[1]})$ is equal to zero. By the chain rule, $\\frac{\\partial}{\\partial \\alpha}f(x_{[1]}) = f'(x_{[1]})^T\\frac{\\partial}{\\partial \\alpha}x_{[1]} = f'(x_{[1]})^Tr_{[0]}$ Setting this expression to zero, we find that $\\alpha$ should be chosen so that $r_{[0]}$ and $f'(x_{[1]})$ are orthogonal (see Figure 6(d))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1f6d39ab461f495399d9d08572979ed5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig6(A, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figure 6: The method of Steepest Descent. (a) Starting at $[-2, -2]^T$, take a step in the direction of steepest descent of $f$. (b) Find the point on surface along the line with direction $r$ that minimizes $f$. (c) This parabola is the intersection of surfaces. The bottommost point is our target. (d) The gradient at the bottommost point is orthogonal to the gradient of the previous step.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is an intuitive reason why we should expect these vectors to be orthogonal at the minimum. Figure 7 shows the gradient vectors at various points along the search line. The slope of the parabola (Figure 6(c)) at any point is equal to the magnitude of the projection of the gradient onto the line (Figure 7, dotted arrows). These projections represent the rate of increase of $f$ as one traverses the search line. $f$ is minimized where the projection is zero — where the gradient is orthogonal to the search line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "47c8f844710a492db814eea30f9d2383",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig7(A, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figure 7: The gradient $f'$ is shown at several locations along the search line (solid arrows). Each gradient’s projection onto the line is also shown (dotted arrows). The gradient vectors represent the direction of steepest increase of $f$, and the projections represent the rate of increase as one traverses the search line. On the search line, $f$ is minimized where the gradient is orthogonal to the search line.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To determine $\\alpha$, note that $f'(x_{[1]}) = -r_{[1]}$, and we have\n",
    "\n",
    "\\begin{align*}\n",
    "r_{[1]}^T r_{[0]} &= 0 \\\\\n",
    "(b - Ax_{[1]})^T r_{[0]} &= 0 \\\\\n",
    "(b - A(x_{[0]} + \\alpha r_{[0]}))^T r_{[0]} &= 0 \\\\\n",
    "(b - Ax_{[0]})^T r_{[0]} - \\alpha (Ar_{[0]})^T r_{[0]} &= 0 \\\\\n",
    "(b - Ax_{[0]})^T r_{[0]} &= \\alpha (Ar_{[0]})^T r_{[0]} \\\\\n",
    "r_{[0]}^T r_{[0]} &= \\alpha r_{[0]}^T (Ar_{[0]}) \\\\\n",
    "\\alpha = \\frac{r_{[0]}^T r_{[0]}}{r_{[0]}^T (Ar_{[0]})}.\n",
    "\\end{align*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ea0fd9133b44f4392cdfd2addd22d82",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig8(A, b, c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center>Figure 8: Here, the method of Steepest Descent starts at $[-2, -2]^T$ and converges at $[2, -2]^T$.</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Putting it all together, the method of Steepest Descent is:\n",
    "\n",
    "\\begin{align}\n",
    "r_{[i]} &= b - Ax_{[i]}, \\tag{10} \\\\\n",
    "\\alpha_{[i]} &= \\frac{r_{[i]}^Tr_{[i]}}{r_{[i]}^TAr_{[i]}}, \\tag{11} \\\\\n",
    "x_{[i+1]} &= x_{[i]} + \\alpha_{[i]}r_{[i]}. \\tag{12}\n",
    "\\end{align}\n",
    "\n",
    "The example is run until it converges in Figure 8. Note the zigzag path, which appears because each gradient is orthogonal to the previous gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<hr style=\"height:2px;border:none;color:#228;background-color:#228;\" />\n",
    "<span style=\"color:#228\">\n",
    "<H3>Steepest Descent vs Gradient Descent</H3>\n",
    "\n",
    "<p>If, instead of computing $\\alpha$ for an optimal line search at each iteration, we simply fix $\\alpha$ at some small value and hope for the best, the method is called Gradient Descent. It reduces the number of products that need to be calculated at each iteration, but typically increases the number of iterations needed to converge. The user now needs to pick a value for $\\alpha$: too large and the iterations will diverge; too small and convergence to $x$ will be slower than necessary, and can become too slow to be practical.\n",
    "\n",
    "<p>Below I have added another interactive figure for you to explore Steepest Descent and Gradient Descent. The only difference between the two methods is that in Steepest Descent 'alpha' is calculated as in equation (11), whereas in Gradient Descent the value is taken from the 'alpha' slider."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d997a149a9e64c96a0710ae876de78cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Canvas(toolbar=Toolbar(toolitems=[('Home', 'Reset original view', 'home', 'home'), ('Back', 'Back to previous …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "axB = fig_B()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "805b08c12c084dda859df19fe7325dad",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Box(children=(Box(children=(Box(children=(FloatSlider(value=3.0, max=10.0, min=-10.0), FloatSlider(value=2.0, …"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sliders_figB(axB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<hr style=\"height:2px;border:none;color:#228;background-color:#228;\" />\n",
    "</html>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The algorithm in equations (10)-(12) requires two matrix-vector multiplications per iteration. The computational cost of Steepest Descent is dominated by matrix-vector products; fortunately, one can be eliminated. By premultiplying both sides of Equation 12 by $-A$ and adding $b$, we have\n",
    "\n",
    "\\begin{equation}\n",
    "r_{[i+1]} = r_{[i]} - \\alpha_{[i]}Ar_{[i]}. \\tag{13}\n",
    "\\end{equation}\n",
    "\n",
    "Although Equation 10 is still needed to compute $r_{[0]})$, Equation 13 can be used for every iteration thereafter. The product $Ar,$ which occurs in both Equations 11 and 13, need only be computed once. The disadvantage of using this recurrence is that the sequence defined by Equation 13 is generated without any feedback from the value of $x_{[i]}$, so that accumulation of floating point roundoff error may cause $x_{[i]}$ to converge to some point near $x$. This effect can be avoided by periodically using Equation 10 to recompute the correct residual.\n",
    "\n",
    "Before analyzing the convergence of Steepest Descent, I must digress to ensure that you have a solid understanding of eigenvectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next: [5. Thinking with Eigenvectors and Eigenvalues](CG05.ipynb)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "latex_envs": {
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 0
  },
  "widgets": {
   "state": {
    "03704111bcc74355b56ac4f58785709e": {
     "views": []
    },
    "0bdce1e088fc4ba9b2b3beafc904da1e": {
     "views": []
    },
    "13ce17674ff44688aeaf8cf762222fbc": {
     "views": []
    },
    "19ac036aed6b4e9590849c26f6b464d3": {
     "views": []
    },
    "1f736a83e94046daa2d9b37ca14d452e": {
     "views": []
    },
    "2d1162a574ed4d1b9271dc56bd88f88f": {
     "views": []
    },
    "3d4167052ed84952b0a110217ab2d90d": {
     "views": []
    },
    "414c21cfdebb440a86502debc9806c10": {
     "views": []
    },
    "41a3b02b8aad4d28bdd2b3e9afafe5e8": {
     "views": []
    },
    "5f48685ee0134659bda6dd382f239048": {
     "views": []
    },
    "631e1da6018e4c71897117304744ebcc": {
     "views": []
    },
    "66b855f9233545f6b07438c1f8f8616f": {
     "views": []
    },
    "7557d08ae726426388ec1b9e103f69c9": {
     "views": []
    },
    "763908ca0007429cbac86b596cf30668": {
     "views": []
    },
    "7b8432f69f04430584843a322f446c26": {
     "views": [
      {
       "cell_index": 15
      }
     ]
    },
    "7bf1236f11b1448eb812494da901adcb": {
     "views": []
    },
    "7f8ea49c8575457592ee472e280d4d1a": {
     "views": []
    },
    "810e61a48baf4f888eb5213c981ff2ba": {
     "views": []
    },
    "844f99e0479c42638193a29e93720217": {
     "views": []
    },
    "91db8deb1c5145cb9a614e7f37a711e7": {
     "views": []
    },
    "9779a415201a43018dd061e80d7e3d70": {
     "views": []
    },
    "9baa580fb23f43c49f0f9690a262b1d2": {
     "views": []
    },
    "9da9ee06ff3a4678a9faee931b353db3": {
     "views": []
    },
    "a052485c09e14d7a93abea2d77f4af3d": {
     "views": []
    },
    "a234523485184e32985ea6b336fa1c72": {
     "views": []
    },
    "b22490977d43445eaf46e029ce4f43ac": {
     "views": []
    },
    "b2813c03940e4b3ebf3ac0deac2cb385": {
     "views": []
    },
    "b784156b503042e0b20aed7ea8440a59": {
     "views": []
    },
    "c8709899bda14dabbca5fa1a2ef20080": {
     "views": []
    },
    "d53fcf02bd8747b89078a2c4ca5e5962": {
     "views": []
    },
    "e0ce5c93a1204cd3a316a42d036990d4": {
     "views": []
    },
    "e3fc202f47c14113b39872da897ea0aa": {
     "views": []
    }
   },
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
